---
title: "Assigning Chinook salmon at Various Read Depth Levels"
output: html_notebook
---

# In Brief

I am going to do GSI between Klamath and Sacto.  This will be pretty
straightforward, because they are quite diverged.  But I want to explore
super-low coverage in this.

**Klamath samples**
    
    - Trinity Fall
    - Trinity Spring
    - Salmon Fall
    - Salmon Spring

That is 64 fish.  We will take the 12 lowest DNA concentration fish out of these
as the fish to be assigned.

**Central Valley samples**

    - Feather Fall
    - Feather Spring
    - Coleman Late Fall
    - San Joaquin
    
That is also 64 fish. Once again, we take the 12 lowest DNA concentration fish
out of these to be the "unknowns" that we will assign.


First, get the sample lists all together.
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
meta <- read_csv("data/wgs-chinook-samples.csv")

sample_list <- meta %>%
  mutate(
    ref_pop = case_when(
      Population %in% c(
        "Coleman Hatchery Late Fall", 
        "Feather River Hatchery Fall", 
        "Feather River Hatchery Spring",
        "San Joaquin River Fall"
        ) ~ "Sacramento",
      Population %in% c(
        "Trinity River Hatchery Fall",
        "Trinity River Hatchery Spring",
        "Salmon River Fall",
        "Salmon River Spring"
        ) ~ "Klamath",
      TRUE ~ NA_character_
    )
  ) %>%
  filter(!is.na(ref_pop)) %>%
  arrange(ref_pop, `Concentration (ng/ul)`) %>%
  group_by(ref_pop) %>%
  mutate(
    group = case_when(
      1:n() <= 12 ~ "mixture",
      TRUE ~ "reference"
    )
  ) %>%
  ungroup() %>%
  select(vcf_name, NMFS_DNA_ID, ref_pop, group) %>%
  arrange(desc(group), ref_pop) %>% 
  mutate(idx = 1:n(), .before = vcf_name)

dir.create("outputs", showWarnings = FALSE)
write_csv(sample_list, file = "outputs/sample_list.csv")


sample_list %>% 
  pull(vcf_name) %>%
  cat(., sep = "\n", file = "config/sample_names.txt")

```

Now, get the FAI file and scaffold groups and things that we need.
```{r}
# read the old chroms and scaffold groups file to put them together

# drop chromosome 28 from it
chroms <- read_tsv("data/chromosomes.tsv") %>%
  mutate(id = sprintf("scaff_group%03d", 1:n()), .before = chrom) %>%
  mutate(
    start = 1,
    stop = num_bases, 
    angsd_chrom = str_replace(chrom, "_", "-")
  ) %>%
  select(-num_bases) %>%
  filter(chrom != "NC_037124.1")
  

scaffs <- read_tsv("data/scaffold_groups.tsv") %>%
  mutate(
    start = 1,
    stop = len,
    angsd_chrom = str_replace(chrom, "_", "-")
  ) %>%
  select(-len, -cumul) %>%
  extract(id, into = "int", regex = "scaff_group([0-9][0-9][0-9])", convert = TRUE) %>%
  mutate(
    int = sprintf("scaff_group%03d", int + 34)
  ) %>%
  rename(id = int)

# put those together
scaff_groups <- bind_rows(
  chroms,
  scaffs
) %>%
  extract(id, into = "num", regex = "scaff_group([0-9][0-9][0-9])", convert = TRUE, remove = FALSE) %>%
  mutate(mh_label = ifelse(num <= 34, num, str_c("Unk-", num))) %>%
  select(-num)

write_tsv(scaff_groups, "config/scaffold_groups.tsv")

```




With that, we are ready to get all the stuff we need to make a Beagle GL file, etc.
```sh
git clone git@github.com:eriqande/mega-post-bcf-exploratory-snakeflows.git
cd mega-post-bcf-exploratory-snakeflows

# now put the VCF into a directory called data
rclone copy --drive-shared-with-me gdrive-rclone:chinook_WGS_processed/aa-DeCorrupted-Single-Big-VCF data

# get a node
srun -c 20 -pty /bin/bash

# activate snakemake conda env
conda activate snakemake-7.7.0

# dry-run
snakemake -np --use-conda --configfile ../config/config.yaml

# which gives us this:
[Thu Feb 23 08:36:26 2023]
localrule all:
    input: results/bcf_cal_chinook/filt_snps05/all/thin_0_0/beagle-gl/beagle-gl.gz
    jobid: 0
    resources: tmpdir=/tmp

Job stats:
job                           count    min threads    max threads
--------------------------  -------  -------------  -------------
all                               1              1              1
bcf2beagle_gl_gather              1              1              1
bcf2beagle_gl_scatter            49              1              1
bcf_samps_and_filt_gather         1              1              1
bcf_samps_and_filt_scatter       49              1              1
total                           101              1              1

# so we run it for real:
snakemake -p --cores 20 --use-conda --configfile ../config/config.yaml

```

- With F_MISSING < 0.3 we get 955313 sites.  
